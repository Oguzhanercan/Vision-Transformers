{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Visual Transformers: Token-based Image Representation and Processing for Computer Vision","metadata":{}},{"cell_type":"markdown","source":"paper -> https://arxiv.org/pdf/2006.03677.pdf","metadata":{}},{"cell_type":"markdown","source":"### notes\n- abstract: convolutions treat all image pixels equally regardless of impor-\ntance; explicitly model all concepts across all images, re-\ngardless of content; and struggle to relate spatially-distant\nconcepts\n\n- Not all pixels are created equal: convolutions uniformly process all\nimage patches regardless of importance. This leads to spa-\ntial inefficiency in both computation and representation\n\n- Not all images have all concepts: All images has low level features like corners, edges but does not have high level feautres like ear shape. This causes unnessecary memory and computation cost.\n\n- a transformer uses\ncontent-aware weights, allowing visual tokens to represent\nvarying concepts.\n\n- 5.2 includes really good recipe for training\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\n!pip install einops\nimport einops\nimport torchvision\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nfrom torchvision import datasets\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport os\nfrom pathlib import Path\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchmetrics import F1Score,Accuracy\nimport matplotlib.pyplot as plt\nimport copy\nimport pytorch_lightning as pl\nimport pandas as pd\nimport seaborn as sn\nfrom IPython.core.display import display\nfrom torch.utils.data import random_split, DataLoader\n!pip install torch-ema\nfrom torch_ema import ExponentialMovingAverage","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:04.802677Z","iopub.execute_input":"2022-09-21T18:46:04.803143Z","iopub.status.idle":"2022-09-21T18:46:31.537103Z","shell.execute_reply.started":"2022-09-21T18:46:04.803046Z","shell.execute_reply":"2022-09-21T18:46:31.535772Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torch-ema\n  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-ema) (1.11.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torch-ema) (4.3.0)\nInstalling collected packages: torch-ema\nSuccessfully installed torch-ema-0.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"feature_map_extractor = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:31.540136Z","iopub.execute_input":"2022-09-21T18:46:31.541617Z","iopub.status.idle":"2022-09-21T18:46:34.556478Z","shell.execute_reply.started":"2022-09-21T18:46:31.541585Z","shell.execute_reply":"2022-09-21T18:46:34.555371Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b4c29988d254e9289c24ec7e814a8e2"}},"metadata":{}}]},{"cell_type":"code","source":"feature_map_extractor = nn.Sequential(feature_map_extractor.conv1,feature_map_extractor.bn1,feature_map_extractor.relu,feature_map_extractor.maxpool,feature_map_extractor.layer1,feature_map_extractor.layer2,feature_map_extractor.layer3)","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:34.558176Z","iopub.execute_input":"2022-09-21T18:46:34.558851Z","iopub.status.idle":"2022-09-21T18:46:34.575140Z","shell.execute_reply.started":"2022-09-21T18:46:34.558808Z","shell.execute_reply":"2022-09-21T18:46:34.574243Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class filter_based_tokenizer(nn.Module):\n    def __init__(self,in_channels:int,out_channels:int):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels,out_channels\n        ,1) \n        self.softmax = nn.Softmax()\n\n    def forward(self,feature_map:torch.Tensor):\n        x = self.conv(feature_map) # HWxC -> HWxL\n        x = self.softmax(x)\n        x = torch.matmul(x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3]),feature_map.view(feature_map.shape[0],feature_map.shape[2]*feature_map.shape[3],feature_map.shape[1])) #CxHxW x LxHxW -> CxL\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:34.577976Z","iopub.execute_input":"2022-09-21T18:46:34.578590Z","iopub.status.idle":"2022-09-21T18:46:34.588418Z","shell.execute_reply.started":"2022-09-21T18:46:34.578552Z","shell.execute_reply":"2022-09-21T18:46:34.587459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class DotProductAttention(nn.Module):\n    def __init__(self,in_channels:int):\n        super().__init__()\n        self.softmax = nn.Softmax(dim = 1)\n        self.conv1 = nn.Conv1d(in_channels,in_channels,1)\n        self.conv2 = nn.Conv1d(in_channels,in_channels,1)\n        self.relu = nn.ReLU()\n    def forward(self,query,key,embed):\n        \n        tfout = torch.matmul(self.softmax(torch.matmul(query,key)),embed) + embed\n        \n        yarrak = self.conv1(einops.rearrange(tfout,\"b n e ->b e n\"))\n        \n        tout = einops.rearrange(tfout,\"b n e ->b e n\") + self.conv2(self.relu(self.conv1(einops.rearrange(tfout,\"b n e ->b e n\"))))\n        return tfout","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:34.589688Z","iopub.execute_input":"2022-09-21T18:46:34.590110Z","iopub.status.idle":"2022-09-21T18:46:34.600210Z","shell.execute_reply.started":"2022-09-21T18:46:34.590074Z","shell.execute_reply":"2022-09-21T18:46:34.599043Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,embed_size,num_heads,dropout = 0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_size = embed_size\n        self.DotProductAttention = DotProductAttention(embed_size)\n\n        self.key = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        self.query = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        self.value = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        \n    def forward(self,embed):\n        batch_size = embed.size(0)\n        query = self.query(embed)\n        key = einops.rearrange(self.query(embed),\"b n e ->b e n\")\n        value = self.value(embed)\n        sdp = self.DotProductAttention(query,key,embed)\n        return sdp\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T18:46:34.601921Z","iopub.execute_input":"2022-09-21T18:46:34.602287Z","iopub.status.idle":"2022-09-21T18:46:34.613220Z","shell.execute_reply.started":"2022-09-21T18:46:34.602250Z","shell.execute_reply":"2022-09-21T18:46:34.612263Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Custom_Dataset():\n\n    def __init__(self, directory):\n        self.path = Path(directory)\n        Path.ls = lambda x: list(x.iterdir())\n        try:\n            files = os.listdir(directory)\n        except:\n            print(\"wrong path\")\n        self.x = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n            torch.FloatTensor) for img in (self.path/files[0]).ls()]\n        self.x = torch.stack(self.x)/255\n        self.y = torch.tensor([0]*len((self.path/files[0]).ls()))\n        \n        for i in range(len(files)-1):\n            try:\n                self.x2 = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n                torch.FloatTensor) for img in (self.path/files[i+1]).ls()]\n            except:\n                return \n            self.x2 = torch.stack(self.x2)/255\n            self.x = torch.cat((self.x, self.x2), 0)\n            self.y = torch.cat((self.y, torch.tensor(\n                [i+1]*len((self.path/files[i+1]).ls()))))\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, i):\n        return self.x[i], self.y[i]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:28.161352Z","iopub.execute_input":"2022-09-21T19:11:28.162428Z","iopub.status.idle":"2022-09-21T19:11:28.176745Z","shell.execute_reply.started":"2022-09-21T19:11:28.162372Z","shell.execute_reply":"2022-09-21T19:11:28.175679Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self,embed_size,num_heads,batch_size,num_blocks,num_class):\n        super().__init__()\n        \n       \n        \n        self.batch_size = batch_size\n        self.num_heads = num_heads\n        self.embed_size = embed_size\n        self.num_blocks =  num_blocks\n        self.num_class = num_class\n\n        self.back_bone = feature_map_extractor\n        self.tiny_block = [MultiHeadAttention(self.embed_size,self.num_heads) for i in range(self.num_blocks)]\n        self.block_seq = nn.Sequential(*self.tiny_block)\n        self.projector = filter_based_tokenizer(embed_size,num_heads)\n        self.pool = nn.AvgPool1d(num_heads)\n        self.linear1 = nn.Linear(self.embed_size,self.num_class*4)\n        self.linear2 = nn.Linear(self.num_class*4,self.num_class)\n\n\n    \n    def forward(self,img):\n        out = self.back_bone(img)\n        out = self.projector(out)\n        out = self.block_seq(out)\n        out = out.permute(0,2,1)\n        out = torch.squeeze(self.pool(out), dim = 2)\n        out = self.linear1(out)\n        out = self.linear2(out)\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:28.640451Z","iopub.execute_input":"2022-09-21T19:11:28.640844Z","iopub.status.idle":"2022-09-21T19:11:28.650812Z","shell.execute_reply.started":"2022-09-21T19:11:28.640813Z","shell.execute_reply":"2022-09-21T19:11:28.649619Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VT(pl.LightningModule):\n    def __init__(self,embed_size,num_heads,batch_size,num_blocks,num_class,learning_rate,directory):\n        super().__init__()\n        \n        #lightning things\n        #self.automatic_optimization = False\n        #things\n        self.dir = directory\n        self.batch_size = batch_size\n        self.val_accuracy = Accuracy()\n        self.test_accuracy = Accuracy()\n        self.epochs = 0\n        #things for model conf\n        self.num_heads = num_heads\n        self.embed_size = embed_size\n        self.num_blocks =  num_blocks\n        self.num_class = num_class\n        self.learning_rate = learning_rate\n        \n        self.model = Model(embed_size,num_heads,batch_size,num_blocks,num_class)\n    \n    def forward(self,img):\n        return self.model(img)\n\n    def num_of_parameters(self,):\n\n        return sum(p.numel() for p in self.parameters())\n\n    def training_epoch_end(self, outputs):\n        optimizer = self.optimizers()\n        \n        if self.epochs < 5:\n            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] *1.75\n        else: \n            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] *0.9875\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.val_accuracy.update(preds, y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.test_accuracy.update(preds, y)\n\n        # Calling self.log will surface up scalars for you in TensorBoard\n        self.log(\"test_loss\", loss, prog_bar=True)\n        self.log(\"test_acc\", self.test_accuracy, prog_bar=True)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n        return optimizer\n \n    def setup(self, stage=None):\n\n        \n        if stage == \"fit\" or stage is None:\n            dataset = Custom_Dataset(self.dir)\n            self.data_train, self.data_val = random_split(dataset, [len(dataset)-5000, 5000])\n\n      \n        if stage == \"test\" or stage is None:\n            self.data_test = MNIST(self.data_dir)\n\n    def train_dataloader(self):\n        return DataLoader(self.data_train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.data_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.data_test, batch_size=self.batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:48.641179Z","iopub.execute_input":"2022-09-21T19:11:48.641579Z","iopub.status.idle":"2022-09-21T19:11:48.657595Z","shell.execute_reply.started":"2022-09-21T19:11:48.641546Z","shell.execute_reply":"2022-09-21T19:11:48.656322Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"directory = \"../input/animal-image-classification-dataset/Animal Image Dataset\"","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:48.816695Z","iopub.execute_input":"2022-09-21T19:11:48.817383Z","iopub.status.idle":"2022-09-21T19:11:48.822709Z","shell.execute_reply.started":"2022-09-21T19:11:48.817348Z","shell.execute_reply":"2022-09-21T19:11:48.821091Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = VT(1024,16,1,4,10,0.01,directory)","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:48.966502Z","iopub.execute_input":"2022-09-21T19:11:48.966880Z","iopub.status.idle":"2022-09-21T19:11:51.039761Z","shell.execute_reply.started":"2022-09-21T19:11:48.966849Z","shell.execute_reply":"2022-09-21T19:11:51.038752Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"swa_callback = pl.callbacks.StochasticWeightAveraging(swa_lrs=0.99985, swa_epoch_start=1)\ntrainer = pl.Trainer(\n    accelerator=\"auto\",\n    devices=1 if torch.cuda.is_available() else None, \n    max_epochs=30,\n    callbacks=[pl.callbacks.progress.TQDMProgressBar(refresh_rate=20),swa_callback],\n    logger=pl.loggers.CSVLogger(save_dir=\"logs/\"),\n)\ntrainer.fit(model)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-21T19:11:51.041820Z","iopub.execute_input":"2022-09-21T19:11:51.042199Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  # Remove the CWD from sys.path while we load stuff.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be96ae95ddb74aecb388161b489c6a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f25495f0003a475fbfa13aed5908e2a7"}},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}