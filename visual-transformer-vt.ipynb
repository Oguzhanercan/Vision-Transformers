{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Visual Transformers: Token-based Image Representation and Processing for Computer Vision"]},{"cell_type":"markdown","metadata":{},"source":["paper -> https://arxiv.org/pdf/2006.03677.pdf"]},{"cell_type":"markdown","metadata":{},"source":["### notes\n","- abstract: convolutions treat all image pixels equally regardless of impor-\n","tance; explicitly model all concepts across all images, re-\n","gardless of content; and struggle to relate spatially-distant\n","concepts\n","\n","- Not all pixels are created equal: convolutions uniformly process all\n","image patches regardless of importance. This leads to spa-\n","tial inefficiency in both computation and representation\n","\n","- Not all images have all concepts: All images has low level features like corners, edges but does not have high level feautres like ear shape. This causes unnessecary memory and computation cost.\n","\n","- a transformer uses\n","content-aware weights, allowing visual tokens to represent\n","varying concepts.\n","\n","- 5.2 includes really good recipe for training\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:34:28.829683Z","iopub.status.busy":"2022-09-26T21:34:28.829271Z","iopub.status.idle":"2022-09-26T21:34:55.183477Z","shell.execute_reply":"2022-09-26T21:34:55.182253Z","shell.execute_reply.started":"2022-09-26T21:34:28.829603Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.4.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting torch-ema\n","  Downloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-ema) (1.11.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torch-ema) (4.3.0)\n","Installing collected packages: torch-ema\n","Successfully installed torch-ema-0.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torch.nn.functional as F\n","!pip install einops\n","import einops\n","import torchvision\n","import torchvision.transforms as transforms\n","import cv2\n","import numpy as np\n","from torchvision import datasets\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import os\n","from pathlib import Path\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","from torchmetrics import F1Score,Accuracy\n","import matplotlib.pyplot as plt\n","import copy\n","import pytorch_lightning as pl\n","import pandas as pd\n","import seaborn as sn\n","from IPython.core.display import display\n","from torch.utils.data import random_split, DataLoader\n","!pip install torch-ema\n","from torch_ema import ExponentialMovingAverage"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:34:55.187862Z","iopub.status.busy":"2022-09-26T21:34:55.185944Z","iopub.status.idle":"2022-09-26T21:35:01.099038Z","shell.execute_reply":"2022-09-26T21:35:01.097917Z","shell.execute_reply.started":"2022-09-26T21:34:55.187820Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"252103af73a94a9591c5dd4182496024","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["feature_map_extractor = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.100978Z","iopub.status.busy":"2022-09-26T21:35:01.100443Z","iopub.status.idle":"2022-09-26T21:35:01.114732Z","shell.execute_reply":"2022-09-26T21:35:01.113897Z","shell.execute_reply.started":"2022-09-26T21:35:01.100943Z"},"trusted":true},"outputs":[],"source":["feature_map_extractor = nn.Sequential(feature_map_extractor.conv1,feature_map_extractor.bn1,feature_map_extractor.relu,feature_map_extractor.maxpool,feature_map_extractor.layer1,feature_map_extractor.layer2,feature_map_extractor.layer3)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.121419Z","iopub.status.busy":"2022-09-26T21:35:01.120825Z","iopub.status.idle":"2022-09-26T21:35:01.134807Z","shell.execute_reply":"2022-09-26T21:35:01.133929Z","shell.execute_reply.started":"2022-09-26T21:35:01.121371Z"},"trusted":true},"outputs":[],"source":["class filter_based_tokenizer(nn.Module):\n","    def __init__(self,in_channels:int,out_channels:int):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels,out_channels\n","        ,1) \n","        self.softmax = nn.Softmax()\n","\n","    def forward(self,feature_map:torch.Tensor):\n","        x = self.conv(feature_map) # HWxC -> HWxL\n","        x = self.softmax(x)\n","        x = torch.matmul(x.view(x.shape[0],x.shape[1],x.shape[2]*x.shape[3]),feature_map.view(feature_map.shape[0],feature_map.shape[2]*feature_map.shape[3],feature_map.shape[1])) #CxHxW x LxHxW -> CxL\n","        return x\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.141946Z","iopub.status.busy":"2022-09-26T21:35:01.139387Z","iopub.status.idle":"2022-09-26T21:35:01.151709Z","shell.execute_reply":"2022-09-26T21:35:01.150838Z","shell.execute_reply.started":"2022-09-26T21:35:01.141908Z"},"trusted":true},"outputs":[],"source":["class DotProductAttention(nn.Module):\n","    def __init__(self,in_channels:int):\n","        super().__init__()\n","        self.softmax = nn.Softmax(dim = 1)\n","        self.conv1 = nn.Conv1d(in_channels,in_channels,1)\n","        self.conv2 = nn.Conv1d(in_channels,in_channels,1)\n","        self.relu = nn.ReLU()\n","    def forward(self,query,key,embed):\n","        \n","        tfout = torch.matmul(self.softmax(torch.matmul(query,key)),embed) + embed   \n","        tfout = einops.rearrange(tfout,\"b n e ->b e n\") + self.conv2(self.relu(self.conv1(einops.rearrange(tfout,\"b n e ->b e n\"))))\n","        tfout_ = einops.rearrange(tfout,\"b n e ->b e n\")\n","\n","        return tfout_"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.158575Z","iopub.status.busy":"2022-09-26T21:35:01.155918Z","iopub.status.idle":"2022-09-26T21:35:01.169599Z","shell.execute_reply":"2022-09-26T21:35:01.168675Z","shell.execute_reply.started":"2022-09-26T21:35:01.158541Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self,embed_size,num_heads,dropout = 0.1):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        self.embed_size = embed_size\n","        self.DotProductAttention = DotProductAttention(embed_size)\n","\n","        self.key = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n","        self.query = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n","        self.value = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n","        \n","    def forward(self,embed):\n","        batch_size = embed.size(0)\n","        query = self.query(embed)\n","        key = einops.rearrange(self.query(embed),\"b n e ->b e n\")\n","        value = self.value(embed)\n","        sdp = self.DotProductAttention(query,key,embed)\n","        return sdp\n","        \n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.176592Z","iopub.status.busy":"2022-09-26T21:35:01.174109Z","iopub.status.idle":"2022-09-26T21:35:01.193188Z","shell.execute_reply":"2022-09-26T21:35:01.192306Z","shell.execute_reply.started":"2022-09-26T21:35:01.176556Z"},"trusted":true},"outputs":[],"source":["class Custom_Dataset():\n","\n","    def __init__(self, directory):\n","        self.path = Path(directory)\n","        Path.ls = lambda x: list(x.iterdir())\n","        try:\n","            files = os.listdir(directory)\n","        except:\n","            print(\"wrong path\")\n","        self.x = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n","            torch.FloatTensor) for img in (self.path/files[0]).ls()]\n","        self.x = torch.stack(self.x)/255\n","        self.y = torch.tensor([0]*len((self.path/files[0]).ls()))\n","        \n","        for i in range(len(files)-1):\n","            try:\n","                self.x2 = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n","                torch.FloatTensor) for img in (self.path/files[i+1]).ls()]\n","            except:\n","                return \n","            self.x2 = torch.stack(self.x2)/255\n","            self.x = torch.cat((self.x, self.x2), 0)\n","            self.y = torch.cat((self.y, torch.tensor(\n","                [i+1]*len((self.path/files[i+1]).ls()))))\n","        \n","    def __len__(self):\n","        return len(self.x)\n","    \n","    def __getitem__(self, i):\n","        return self.x[i], self.y[i]\n","    "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.200713Z","iopub.status.busy":"2022-09-26T21:35:01.198070Z","iopub.status.idle":"2022-09-26T21:35:01.216496Z","shell.execute_reply":"2022-09-26T21:35:01.215436Z","shell.execute_reply.started":"2022-09-26T21:35:01.200678Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self,embed_size,num_heads,batch_size,num_blocks,num_class):\n","        super().__init__()\n","        \n","       \n","        \n","        self.batch_size = batch_size\n","        self.num_heads = num_heads\n","        self.embed_size = embed_size\n","        self.num_blocks =  num_blocks\n","        self.num_class = num_class\n","\n","        self.back_bone = feature_map_extractor\n","        self.tiny_block = [MultiHeadAttention(self.embed_size,self.num_heads) for i in range(self.num_blocks)]\n","        self.block_seq = nn.Sequential(*self.tiny_block)\n","        self.projector = filter_based_tokenizer(embed_size,num_heads)\n","        self.pool = nn.AvgPool1d(num_heads)\n","        self.linear1 = nn.Linear(self.embed_size,self.num_class*4)\n","        self.linear2 = nn.Linear(self.num_class*4,self.num_class)\n","        self.softmax = nn.LogSoftmax(dim = 1)\n","        self.layernorm1 = nn.LayerNorm(self.embed_size)\n","        self.layernorm2 = nn.LayerNorm(self.num_class*4)\n","        self.layernorm3 = nn.LayerNorm(self.num_class)\n","    \n","    def forward(self,img):\n","        out = self.back_bone(img)\n","        out = self.projector(out)\n","        out = self.block_seq(out)\n","        out = out.permute(0,2,1)\n","        out = torch.squeeze(self.pool(out), dim = 2)\n","        out = self.layernorm1(out)\n","        out = self.linear1(out)\n","        out = self.layernorm2(out)\n","        out = self.linear2(out)\n","        out = self.layernorm3(out)\n","        out = self.softmax(out)\n","        \n","        return out\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:35:01.257553Z","iopub.status.busy":"2022-09-26T21:35:01.255039Z","iopub.status.idle":"2022-09-26T21:36:33.317288Z","shell.execute_reply":"2022-09-26T21:36:33.316287Z","shell.execute_reply.started":"2022-09-26T21:35:01.257515Z"},"trusted":true},"outputs":[],"source":["dataset = Custom_Dataset(\"../input/animal-image-classification-dataset/Animal Image Dataset\")\n","data_train, data_val = random_split(dataset, [len(dataset)-5000, 5000])"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:36:33.319198Z","iopub.status.busy":"2022-09-26T21:36:33.318825Z","iopub.status.idle":"2022-09-26T21:36:33.324306Z","shell.execute_reply":"2022-09-26T21:36:33.323307Z","shell.execute_reply.started":"2022-09-26T21:36:33.319159Z"},"trusted":true},"outputs":[],"source":["trainloader =DataLoader(data_train, batch_size=8,drop_last=True)\n","validloader = DataLoader(data_val, batch_size=8,drop_last=True)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:55:51.215722Z","iopub.status.busy":"2022-09-26T21:55:51.215348Z","iopub.status.idle":"2022-09-26T21:55:51.220993Z","shell.execute_reply":"2022-09-26T21:55:51.219514Z","shell.execute_reply.started":"2022-09-26T21:55:51.215692Z"},"trusted":true},"outputs":[],"source":["device = \"cuda\"\n","batch_size = 8\n","lr = 0.0001\n","epochs = 30"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:55:51.486077Z","iopub.status.busy":"2022-09-26T21:55:51.485715Z","iopub.status.idle":"2022-09-26T21:55:53.693455Z","shell.execute_reply":"2022-09-26T21:55:53.692477Z","shell.execute_reply.started":"2022-09-26T21:55:51.486046Z"},"trusted":true},"outputs":[],"source":["model = Model(1024,16,1,4,12).to(device)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:55:53.695936Z","iopub.status.busy":"2022-09-26T21:55:53.695548Z","iopub.status.idle":"2022-09-26T21:55:53.712860Z","shell.execute_reply":"2022-09-26T21:55:53.711815Z","shell.execute_reply.started":"2022-09-26T21:55:53.695890Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n","criterion = nn.NLLLoss()\n","ema = ExponentialMovingAverage(model.parameters(), decay=0.99985)\n","f1 = F1Score().to(device)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-09-26T21:56:25.058663Z","iopub.status.busy":"2022-09-26T21:56:25.058281Z","iopub.status.idle":"2022-09-26T22:05:53.354840Z","shell.execute_reply":"2022-09-26T22:05:53.353811Z","shell.execute_reply.started":"2022-09-26T21:56:25.058632Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch - 1 Started\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # Remove the CWD from sys.path while we load stuff.\n"]},{"name":"stdout","output_type":"stream","text":["F1 Score for train: 0.2574257254600525, F1 Score for validation: 0.07440000027418137 \n","Epoch: 1 \tTraining Loss: 1.636816 \tValidation Loss: 3.033146\n","Validation loss decreased (inf --> 3.033146).  Saving model ...\n","Epoch - 2 Started\n","F1 Score for train: 0.31435641646385193, F1 Score for validation: 0.24879999458789825 \n","Epoch: 2 \tTraining Loss: 1.571948 \tValidation Loss: 1.610839\n","Validation loss decreased (3.033146 --> 1.610839).  Saving model ...\n","Epoch - 3 Started\n","F1 Score for train: 0.408415824174881, F1 Score for validation: 0.3091999888420105 \n","Epoch: 3 \tTraining Loss: 1.456517 \tValidation Loss: 1.556321\n","Validation loss decreased (1.610839 --> 1.556321).  Saving model ...\n","Epoch - 4 Started\n","F1 Score for train: 0.47524750232696533, F1 Score for validation: 0.3879999816417694 \n","Epoch: 4 \tTraining Loss: 1.327041 \tValidation Loss: 1.492367\n","Validation loss decreased (1.556321 --> 1.492367).  Saving model ...\n","Epoch - 5 Started\n","F1 Score for train: 0.5779702663421631, F1 Score for validation: 0.47119998931884766 \n","Epoch: 5 \tTraining Loss: 1.179354 \tValidation Loss: 1.384385\n","Validation loss decreased (1.492367 --> 1.384385).  Saving model ...\n","Epoch - 6 Started\n","F1 Score for train: 0.6596534252166748, F1 Score for validation: 0.5239999890327454 \n","Epoch: 6 \tTraining Loss: 1.038792 \tValidation Loss: 1.296961\n","Validation loss decreased (1.384385 --> 1.296961).  Saving model ...\n","Epoch - 7 Started\n","F1 Score for train: 0.733910858631134, F1 Score for validation: 0.5590000152587891 \n","Epoch: 7 \tTraining Loss: 0.899027 \tValidation Loss: 1.269699\n","Validation loss decreased (1.296961 --> 1.269699).  Saving model ...\n","Epoch - 8 Started\n","F1 Score for train: 0.7747524380683899, F1 Score for validation: 0.5735999941825867 \n","Epoch: 8 \tTraining Loss: 0.794602 \tValidation Loss: 1.265624\n","Validation loss decreased (1.269699 --> 1.265624).  Saving model ...\n","Epoch - 9 Started\n","F1 Score for train: 0.8329207897186279, F1 Score for validation: 0.5997999906539917 \n","Epoch: 9 \tTraining Loss: 0.660774 \tValidation Loss: 1.224329\n","Validation loss decreased (1.265624 --> 1.224329).  Saving model ...\n","Epoch - 10 Started\n","F1 Score for train: 0.8663366436958313, F1 Score for validation: 0.6240000128746033 \n","Epoch: 10 \tTraining Loss: 0.595889 \tValidation Loss: 1.193507\n","Validation loss decreased (1.224329 --> 1.193507).  Saving model ...\n","Epoch - 11 Started\n","F1 Score for train: 0.8787128329277039, F1 Score for validation: 0.5931999683380127 \n","Epoch: 11 \tTraining Loss: 0.550758 \tValidation Loss: 1.340506\n","Epoch - 12 Started\n","F1 Score for train: 0.9022276997566223, F1 Score for validation: 0.6177999973297119 \n","Epoch: 12 \tTraining Loss: 0.499216 \tValidation Loss: 1.240866\n","Epoch - 13 Started\n","F1 Score for train: 0.9133663177490234, F1 Score for validation: 0.6236000061035156 \n","Epoch: 13 \tTraining Loss: 0.461149 \tValidation Loss: 1.244679\n","Epoch - 14 Started\n","F1 Score for train: 0.9282178282737732, F1 Score for validation: 0.6096000075340271 \n","Epoch: 14 \tTraining Loss: 0.418503 \tValidation Loss: 1.286392\n","Epoch - 15 Started\n","F1 Score for train: 0.9245049357414246, F1 Score for validation: 0.6293999552726746 \n","Epoch: 15 \tTraining Loss: 0.398388 \tValidation Loss: 1.244871\n","Epoch - 16 Started\n","F1 Score for train: 0.9183168411254883, F1 Score for validation: 0.6520000100135803 \n","Epoch: 16 \tTraining Loss: 0.402917 \tValidation Loss: 1.168375\n","Validation loss decreased (1.193507 --> 1.168375).  Saving model ...\n","Epoch - 17 Started\n","F1 Score for train: 0.9504950046539307, F1 Score for validation: 0.6448000073432922 \n","Epoch: 17 \tTraining Loss: 0.334667 \tValidation Loss: 1.218801\n","Epoch - 18 Started\n","F1 Score for train: 0.9368811845779419, F1 Score for validation: 0.6157999634742737 \n","Epoch: 18 \tTraining Loss: 0.357815 \tValidation Loss: 1.332399\n","Epoch - 19 Started\n","F1 Score for train: 0.9344059228897095, F1 Score for validation: 0.6595999598503113 \n","Epoch: 19 \tTraining Loss: 0.344436 \tValidation Loss: 1.184302\n","Epoch - 20 Started\n","F1 Score for train: 0.9504950046539307, F1 Score for validation: 0.6417999863624573 \n","Epoch: 20 \tTraining Loss: 0.312209 \tValidation Loss: 1.274644\n","Epoch - 21 Started\n","F1 Score for train: 0.9566831588745117, F1 Score for validation: 0.6552000045776367 \n","Epoch: 21 \tTraining Loss: 0.290341 \tValidation Loss: 1.221168\n","Epoch - 22 Started\n","F1 Score for train: 0.9603960514068604, F1 Score for validation: 0.6549999713897705 \n","Epoch: 22 \tTraining Loss: 0.289505 \tValidation Loss: 1.215004\n","Epoch - 23 Started\n","F1 Score for train: 0.9492574334144592, F1 Score for validation: 0.6279999613761902 \n","Epoch: 23 \tTraining Loss: 0.297734 \tValidation Loss: 1.329161\n","Epoch - 24 Started\n","F1 Score for train: 0.978960394859314, F1 Score for validation: 0.6347999572753906 \n","Epoch: 24 \tTraining Loss: 0.230461 \tValidation Loss: 1.307110\n","Epoch - 25 Started\n","F1 Score for train: 0.9603960514068604, F1 Score for validation: 0.6459999680519104 \n","Epoch: 25 \tTraining Loss: 0.265121 \tValidation Loss: 1.283586\n","Epoch - 26 Started\n","F1 Score for train: 0.9678217768669128, F1 Score for validation: 0.6283999681472778 \n","Epoch: 26 \tTraining Loss: 0.238691 \tValidation Loss: 1.376669\n","Epoch - 27 Started\n","F1 Score for train: 0.969059407711029, F1 Score for validation: 0.6423999667167664 \n","Epoch: 27 \tTraining Loss: 0.235309 \tValidation Loss: 1.327629\n","Epoch - 28 Started\n","F1 Score for train: 0.9764851331710815, F1 Score for validation: 0.6485999822616577 \n","Epoch: 28 \tTraining Loss: 0.208750 \tValidation Loss: 1.289251\n","Epoch - 29 Started\n","F1 Score for train: 0.969059407711029, F1 Score for validation: 0.6337999701499939 \n","Epoch: 29 \tTraining Loss: 0.217280 \tValidation Loss: 1.343701\n","Epoch - 30 Started\n","F1 Score for train: 0.9863861203193665, F1 Score for validation: 0.649399995803833 \n","Epoch: 30 \tTraining Loss: 0.173400 \tValidation Loss: 1.285858\n"]}],"source":["device = \"cuda\"\n","valid_loss_min = np.Inf\n","for i in range(epochs):\n","    print(\"Epoch - {} Started\".format(i+1))\n","\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    train_score = 0.0\n","    val_score = 0.0\n","    model.train()\n","    for data, target in trainloader:\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()*data.size(0)\n","        train_score = train_score +f1(output,target)\n","    model.eval()\n","    with ema.average_parameters():\n","        for data, target in validloader:\n","            data, target = data.to(device), target.to(device)\n","            with torch.no_grad():\n","                output = model(data)\n","            loss = criterion(output, target)\n","            valid_loss += loss.item()*data.size(0)\n","            val_score = val_score + f1(output,target)\n","\n","    \n","    train_loss = train_loss/len(trainloader.sampler)\n","    valid_loss = valid_loss/len(validloader.sampler)\n","    train_score = batch_size*train_score/len(trainloader.sampler)\n","    val_score = batch_size*val_score/len(validloader.sampler)\n","    \n","    if epochs < 5:\n","        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] *1.75\n","    else: \n","        optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] *0.9875\n","    \n","    \n","    print(f\"F1 Score for train: {train_score}, F1 Score for validation: {val_score} \")\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        i+1, train_loss, valid_loss))\n","    ema.update()\n","    if valid_loss <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            valid_loss_min,\n","            valid_loss))\n","        best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        valid_loss_min = valid_loss\n","torch.save(best_model_wts, 'model.pt')"]},{"cell_type":"markdown","metadata":{},"source":["### Notes after training:\n","- Model stacked too many times to local minimum, it was hard to save it.\n","- As authors says, overfitting is a hugeeeee problem."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
