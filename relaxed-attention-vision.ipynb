{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Relaxed Attention for Transformer Models","metadata":{}},{"cell_type":"markdown","source":"paper -> https://arxiv.org/pdf/2209.09735.pdf\n\n### Notes\n\n- abstract: The powerful modeling capabilities of all-attention-based transformer architec-\ntures often cause overfitting\n\n- instead of swin, I implemented vanilla ViT with relaxed attention\n- followed the training receipe at the paper, additionally lr scheduler added","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\n!pip install einops\nimport einops\nimport torchvision\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nfrom torchvision import datasets\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport os\nfrom pathlib import Path\nimport torch\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom torchmetrics import F1Score,Accuracy\nimport matplotlib.pyplot as plt\nimport copy\nimport pytorch_lightning as pl\nimport pandas as pd\nimport seaborn as sn\nfrom IPython.core.display import display\nfrom torch.utils.data import random_split, DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T12:21:59.499279Z","iopub.execute_input":"2022-09-27T12:21:59.500476Z","iopub.status.idle":"2022-09-27T12:22:21.456031Z","shell.execute_reply.started":"2022-09-27T12:21:59.500345Z","shell.execute_reply":"2022-09-27T12:22:21.454720Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nInstalling collected packages: einops\nSuccessfully installed einops-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"valid_per = 0.20\nbatch_size = 1\nepochs = 20\nbeta1 = 0.9\nbeta2 = 0.990\nweight_decay = 0.3\nembed_size =768\nnum_heads = 8\npatch_size = 12\nin_channels = 3\nnum_encoders = 8\nnum_class = 12\nrelaxation_coeff = 0.03\ndevice = \"cuda\"\nbatch_size = 8\nlearning_rate = 0.000125\nepochs = 30\nlabel_smooth = 0.1","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:14.114825Z","iopub.execute_input":"2022-09-27T13:00:14.115278Z","iopub.status.idle":"2022-09-27T13:00:14.125251Z","shell.execute_reply.started":"2022-09-27T13:00:14.115215Z","shell.execute_reply":"2022-09-27T13:00:14.123693Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class Project(nn.Module):\n    def __init__(self,patch_size:int,in_channels:int,embed_size:int,batch_size:int):\n        super().__init__()\n        self.patch_size = patch_size\n        self.batch_size = batch_size \n        self.embed_size = embed_size # embed size is the size of linearly projected patch of image\n        self.in_channels = in_channels # channel size of image, 1 for grayscale, 3 for colored image\n\n        self.linear = nn.Linear(self.in_channels*self.patch_size**2,self.embed_size)\n        self.class_token = nn.Parameter(torch.randn(self.batch_size,1,embed_size))\n        self.position_embed = nn.Parameter(torch.randn(self.patch_size**2+1,self.embed_size))\n\n    def forward(self,x:int): # num_batch x in_channel x width x height -> num_bahch x  num_patch x embed_size\n        out = einops.rearrange(x,\"b c (h px) (w py) -> b (h w) (c px py)\",px = self.patch_size, py = self.patch_size)\n        out = self.linear(out)\n        out = torch.cat([out,self.class_token],dim = 1)\n        out = out + self.position_embed\n        return out\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T12:22:21.474138Z","iopub.execute_input":"2022-09-27T12:22:21.475535Z","iopub.status.idle":"2022-09-27T12:22:21.488064Z","shell.execute_reply.started":"2022-09-27T12:22:21.475505Z","shell.execute_reply":"2022-09-27T12:22:21.486692Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class DotProductAttention(nn.Module):\n    def __init__(self,relaxation_coeff,patch_size):\n        super().__init__()\n        self.softmax = nn.Softmax(dim = 1)\n        self.relaxation = Relaxation(relaxation_coeff,patch_size)\n    def forward(self,query,key,value): \n        relaxation = self.relaxation(self.softmax(torch.matmul(query,key)/key.size(dim = 2)**(1/2)))\n        sdp = torch.matmul(relaxation,value)\n        return sdp","metadata":{"execution":{"iopub.status.busy":"2022-09-27T12:22:21.492687Z","iopub.execute_input":"2022-09-27T12:22:21.494195Z","iopub.status.idle":"2022-09-27T12:22:21.504829Z","shell.execute_reply.started":"2022-09-27T12:22:21.494149Z","shell.execute_reply":"2022-09-27T12:22:21.503219Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Relaxation(nn.Module):\n    def __init__(self,relaxation_coeff:float,patch_size:int):\n        super().__init__()\n        self.relaxation_coeff = relaxation_coeff\n        self.T = (torch.ones((patch_size**2+1,patch_size**2+1))/patch_size).to(\"cuda\")\n\n    def forward(self,embed:torch.Tensor):\n        return ((1-self.relaxation_coeff)*embed + self.relaxation_coeff*self.T)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T12:22:21.506655Z","iopub.execute_input":"2022-09-27T12:22:21.508341Z","iopub.status.idle":"2022-09-27T12:22:21.523344Z","shell.execute_reply.started":"2022-09-27T12:22:21.508298Z","shell.execute_reply":"2022-09-27T12:22:21.521638Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self,embed_size,num_heads,relaxation_coeff,patch_size,dropout = 0.2):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_size = embed_size\n        self.DotProductAttention = DotProductAttention(relaxation_coeff,patch_size)\n       \n\n        self.key = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        self.query = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        self.value = nn.Linear(self.embed_size,self.num_heads* self.embed_size,bias = False)\n        \n        self.linear = nn.Linear(self.num_heads*self.embed_size,embed_size,bias = False)\n        self.layer_norm = nn.LayerNorm(self.embed_size, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,embed):\n        batch_size = embed.size(0)\n        query = self.query(embed)\n        key = einops.rearrange(self.query(embed),\"b n e ->b e n\")\n        value = self.value(embed)\n        sdp = self.DotProductAttention(query,key,value)\n        return self.linear(self.dropout(sdp))\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:29.320400Z","iopub.execute_input":"2022-09-27T13:00:29.320845Z","iopub.status.idle":"2022-09-27T13:00:29.335049Z","shell.execute_reply.started":"2022-09-27T13:00:29.320813Z","shell.execute_reply":"2022-09-27T13:00:29.333336Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self,embed_size,num_heads,relaxation_coeff,patch_size,dropout = 0.2):\n        super().__init__()\n        self.embed_size = embed_size\n        self.num_heads = num_heads\n        self.dropout =dropout\n        self.mha = MultiHeadAttention(768,8,relaxation_coeff,patch_size)\n        self.Linear1 = nn.Linear(self.embed_size,self.embed_size*4,bias=False)\n        self.Linear2 = nn.Linear(self.embed_size*4,self.embed_size,bias = False)\n        self.gelu = nn.GELU()\n        self.layer_norm1 = nn.LayerNorm(self.embed_size,eps = 1e-6)\n        self.layer_norm2 = nn.LayerNorm(self.embed_size,eps = 1e-6)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self,embed:torch.Tensor):\n        embed = embed + self.mha(self.layer_norm1(embed))\n        embed = embed+ self.Linear2(self.dropout(self.gelu(self.Linear1(self.dropout(self.layer_norm2(embed))))))\n        return embed\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:32.853310Z","iopub.execute_input":"2022-09-27T13:00:32.853772Z","iopub.status.idle":"2022-09-27T13:00:32.865217Z","shell.execute_reply.started":"2022-09-27T13:00:32.853742Z","shell.execute_reply":"2022-09-27T13:00:32.863399Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,embed_size,num_heads,patch_size,in_channels,batch_size,num_encoders,num_class,device,relaxation_coeff):\n        super().__init__()\n        self.device = device\n        self.num_heads = num_heads\n        self.embed_size = embed_size\n        self.patch_size =patch_size\n        self.in_channels = in_channels\n        self.batch_size = batch_size\n        self.num_encoders = num_encoders\n        self.num_class = num_class\n        self.proj = Project(self.patch_size,self.in_channels,self.embed_size,self.batch_size)\n\n        self.tiny_block = [EncoderBlock(self.embed_size,self.num_heads,relaxation_coeff,patch_size) for i in range(self.num_encoders)]\n        self.block_seq = nn.Sequential(*self.tiny_block)\n        #self.block_seq = EncoderBlock(self.embed_size,self.num_heads,relaxation_coeff,patch_size)\n        self.linear1 = nn.Linear(self.embed_size,self.num_class*4)\n        self.linear2 = nn.Linear(self.num_class*4,self.num_class)\n        self.layernorm1 = nn.LayerNorm(num_class*4)\n        self.layernorm2 = nn.LayerNorm(num_class)\n        self.logsoftmax = nn.LogSoftmax(dim = 0)\n    def num_of_parameters(self,):\n\n        return sum(p.numel() for p in self.parameters())\n    \n    def forward(self,img):\n        out = self.proj(img)\n        out = self.block_seq(out)\n        out = self.linear1(torch.squeeze(torch.index_select(out,1,torch.tensor([self.patch_size**2]).to(self.device))))\n        out = self.layernorm1(out)\n        out = self.linear2(out)\n        out = self.layernorm2(out)\n        out = self.logsoftmax(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:40.318299Z","iopub.execute_input":"2022-09-27T13:00:40.318726Z","iopub.status.idle":"2022-09-27T13:00:40.334103Z","shell.execute_reply.started":"2022-09-27T13:00:40.318696Z","shell.execute_reply":"2022-09-27T13:00:40.332593Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"class Custom_Dataset():\n\n    def __init__(self, directory):\n        self.path = Path(directory)\n        Path.ls = lambda x: list(x.iterdir())\n        try:\n            files = os.listdir(directory)\n        except:\n            print(\"wrong path\")\n        self.x = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n            torch.FloatTensor) for img in (self.path/files[0]).ls()]\n        self.x = torch.stack(self.x)/255\n        self.y = torch.tensor([0]*len((self.path/files[0]).ls()))\n        \n        for i in range(len(files)-1):\n            try:\n                self.x2 = [torch.tensor(np.transpose(np.array(Image.open(img).resize((144,144)))[:, :, :3], (2, 0, 1))).type(\n                torch.FloatTensor) for img in (self.path/files[i+1]).ls()]\n            except:\n                return \n            self.x2 = torch.stack(self.x2)/255\n            self.x = torch.cat((self.x, self.x2), 0)\n            self.y = torch.cat((self.y, torch.tensor(\n                [i+1]*len((self.path/files[i+1]).ls()))))\n        \n    def __len__(self):\n        return len(self.x)\n    \n    def __getitem__(self, i):\n        return self.x[i], self.y[i]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:40.942676Z","iopub.execute_input":"2022-09-27T13:00:40.943120Z","iopub.status.idle":"2022-09-27T13:00:40.958830Z","shell.execute_reply.started":"2022-09-27T13:00:40.943089Z","shell.execute_reply":"2022-09-27T13:00:40.957042Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"dataset = Custom_Dataset(\"../input/animal-image-classification-dataset/Animal Image Dataset\")\ndata_train, data_val = random_split(dataset, [len(dataset)-3000, 3000])","metadata":{"execution":{"iopub.status.busy":"2022-09-27T12:25:08.009843Z","iopub.execute_input":"2022-09-27T12:25:08.010708Z","iopub.status.idle":"2022-09-27T12:26:56.796851Z","shell.execute_reply.started":"2022-09-27T12:25:08.010676Z","shell.execute_reply":"2022-09-27T12:26:56.795455Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainloader =DataLoader(data_train, batch_size=8,drop_last=True)\nvalidloader = DataLoader(data_val, batch_size=8,drop_last=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:46.343852Z","iopub.execute_input":"2022-09-27T13:00:46.344296Z","iopub.status.idle":"2022-09-27T13:00:46.350599Z","shell.execute_reply.started":"2022-09-27T13:00:46.344240Z","shell.execute_reply":"2022-09-27T13:00:46.349067Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"model = Transformer(embed_size,num_heads,patch_size,in_channels,batch_size,num_encoders,num_class,device,relaxation_coeff).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:46.977733Z","iopub.execute_input":"2022-09-27T13:00:46.978717Z","iopub.status.idle":"2022-09-27T13:00:49.189889Z","shell.execute_reply.started":"2022-09-27T13:00:46.978668Z","shell.execute_reply":"2022-09-27T13:00:49.188492Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"f1 = F1Score().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:49.192691Z","iopub.execute_input":"2022-09-27T13:00:49.193241Z","iopub.status.idle":"2022-09-27T13:00:49.203185Z","shell.execute_reply.started":"2022-09-27T13:00:49.193180Z","shell.execute_reply":"2022-09-27T13:00:49.201541Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class LabelSmoothing_NLLL(nn.Module):\n    \"\"\"NLL loss with label smoothing.\n    \"\"\"\n    def __init__(self, smoothing=0.0):\n        \"\"\"Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        \"\"\"\n        super(LabelSmoothing_NLLL, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:50.878016Z","iopub.execute_input":"2022-09-27T13:00:50.879538Z","iopub.status.idle":"2022-09-27T13:00:50.889163Z","shell.execute_reply.started":"2022-09-27T13:00:50.879494Z","shell.execute_reply":"2022-09-27T13:00:50.887376Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"criterion = LabelSmoothing_NLLL(smoothing=label_smooth)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:51.249517Z","iopub.execute_input":"2022-09-27T13:00:51.249953Z","iopub.status.idle":"2022-09-27T13:00:51.256559Z","shell.execute_reply.started":"2022-09-27T13:00:51.249922Z","shell.execute_reply":"2022-09-27T13:00:51.255034Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate,betas = (beta1,beta2),weight_decay=weight_decay)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:00:58.569214Z","iopub.execute_input":"2022-09-27T13:00:58.569625Z","iopub.status.idle":"2022-09-27T13:00:58.578614Z","shell.execute_reply.started":"2022-09-27T13:00:58.569592Z","shell.execute_reply":"2022-09-27T13:00:58.576871Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\"\nvalid_loss_min = np.Inf\nfor i in range(epochs):\n    print(\"Epoch - {} Started\".format(i+1))\n\n    train_loss = 0.0\n    valid_loss = 0.0\n    train_score = 0.0\n    val_score = 0.0\n    model.train()\n    for data, target in trainloader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        train_score = train_score +f1(output,target)\n    \n    model.eval()\n    \n    for data, target in validloader:\n        data, target = data.to(device), target.to(device)\n        with torch.no_grad():\n            output = model(data)\n        loss = criterion(output, target)\n        valid_loss += loss.item()*data.size(0)\n        val_score = val_score + f1(output,target)\n\n    train_loss = train_loss/len(trainloader.sampler)\n    valid_loss = valid_loss/len(validloader.sampler)\n    train_score = batch_size*train_score/len(trainloader.sampler)\n    val_score = batch_size*val_score/len(validloader.sampler)\n\n    print(f\"F1 Score for train: {train_score}, F1 Score for validation: {val_score} \")\n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        i+1, train_loss, valid_loss))\n\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n            valid_loss_min,\n            valid_loss))\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n        valid_loss_min = valid_loss\ntorch.save(best_model_wts, 'model.pt')","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:01:00.595164Z","iopub.execute_input":"2022-09-27T13:01:00.595611Z","iopub.status.idle":"2022-09-27T13:42:47.683506Z","shell.execute_reply.started":"2022-09-27T13:01:00.595563Z","shell.execute_reply":"2022-09-27T13:42:47.681448Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch - 1 Started\nF1 Score for train: 0.20975783467292786, F1 Score for validation: 0.24799999594688416 \nEpoch: 1 \tTraining Loss: 2.103555 \tValidation Loss: 2.089716\nValidation loss decreased (inf --> 2.089716).  Saving model ...\nEpoch - 2 Started\nF1 Score for train: 0.1905270665884018, F1 Score for validation: 0.21833333373069763 \nEpoch: 2 \tTraining Loss: 2.090135 \tValidation Loss: 2.088041\nValidation loss decreased (2.089716 --> 2.088041).  Saving model ...\nEpoch - 3 Started\nF1 Score for train: 0.20477208495140076, F1 Score for validation: 0.21466666460037231 \nEpoch: 3 \tTraining Loss: 2.087483 \tValidation Loss: 2.086394\nValidation loss decreased (2.088041 --> 2.086394).  Saving model ...\nEpoch - 4 Started\nF1 Score for train: 0.2004985809326172, F1 Score for validation: 0.21466666460037231 \nEpoch: 4 \tTraining Loss: 2.084855 \tValidation Loss: 2.084636\nValidation loss decreased (2.086394 --> 2.084636).  Saving model ...\nEpoch - 5 Started\nF1 Score for train: 0.18660968542099, F1 Score for validation: 0.21466666460037231 \nEpoch: 5 \tTraining Loss: 2.082845 \tValidation Loss: 2.083203\nValidation loss decreased (2.084636 --> 2.083203).  Saving model ...\nEpoch - 6 Started\nF1 Score for train: 0.1844729334115982, F1 Score for validation: 0.18533332645893097 \nEpoch: 6 \tTraining Loss: 2.081569 \tValidation Loss: 2.082677\nValidation loss decreased (2.083203 --> 2.082677).  Saving model ...\nEpoch - 7 Started\nF1 Score for train: 0.18696580827236176, F1 Score for validation: 0.18833333253860474 \nEpoch: 7 \tTraining Loss: 2.080809 \tValidation Loss: 2.080968\nValidation loss decreased (2.082677 --> 2.080968).  Saving model ...\nEpoch - 8 Started\nF1 Score for train: 0.16096866130828857, F1 Score for validation: 0.09300000220537186 \nEpoch: 8 \tTraining Loss: 2.081071 \tValidation Loss: 2.079854\nValidation loss decreased (2.080968 --> 2.079854).  Saving model ...\nEpoch - 9 Started\nF1 Score for train: 0.13746438920497894, F1 Score for validation: 0.21066667139530182 \nEpoch: 9 \tTraining Loss: 2.080024 \tValidation Loss: 2.081595\nEpoch - 10 Started\nF1 Score for train: 0.1467236429452896, F1 Score for validation: 0.09300000220537186 \nEpoch: 10 \tTraining Loss: 2.080961 \tValidation Loss: 2.080041\nEpoch - 11 Started\nF1 Score for train: 0.13354700803756714, F1 Score for validation: 0.14933332800865173 \nEpoch: 11 \tTraining Loss: 2.080019 \tValidation Loss: 2.080619\nEpoch - 12 Started\nF1 Score for train: 0.14138177037239075, F1 Score for validation: 0.09300000220537186 \nEpoch: 12 \tTraining Loss: 2.080572 \tValidation Loss: 2.080161\nEpoch - 13 Started\nF1 Score for train: 0.14102564752101898, F1 Score for validation: 0.18766666948795319 \nEpoch: 13 \tTraining Loss: 2.080509 \tValidation Loss: 2.080317\nEpoch - 14 Started\nF1 Score for train: 0.13354700803756714, F1 Score for validation: 0.1263333261013031 \nEpoch: 14 \tTraining Loss: 2.080085 \tValidation Loss: 2.080500\nEpoch - 15 Started\nF1 Score for train: 0.1378205120563507, F1 Score for validation: 0.18799999356269836 \nEpoch: 15 \tTraining Loss: 2.079919 \tValidation Loss: 2.080436\nEpoch - 16 Started\nF1 Score for train: 0.1517094075679779, F1 Score for validation: 0.2173333317041397 \nEpoch: 16 \tTraining Loss: 2.079828 \tValidation Loss: 2.080558\nEpoch - 17 Started\nF1 Score for train: 0.15918803215026855, F1 Score for validation: 0.2173333317041397 \nEpoch: 17 \tTraining Loss: 2.079780 \tValidation Loss: 2.080502\nEpoch - 18 Started\nF1 Score for train: 0.1755698025226593, F1 Score for validation: 0.2173333317041397 \nEpoch: 18 \tTraining Loss: 2.079742 \tValidation Loss: 2.080483\nEpoch - 19 Started\nF1 Score for train: 0.1855413168668747, F1 Score for validation: 0.2173333317041397 \nEpoch: 19 \tTraining Loss: 2.079694 \tValidation Loss: 2.080279\nEpoch - 20 Started\nF1 Score for train: 0.18233618140220642, F1 Score for validation: 0.18799999356269836 \nEpoch: 20 \tTraining Loss: 2.079635 \tValidation Loss: 2.080007\nEpoch - 21 Started\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/2032847016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_score\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}